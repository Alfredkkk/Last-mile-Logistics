{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b99590c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drl_comodal.py\n",
    "# Deep RL for joint ride-hailing & package delivery (no zoning)\n",
    "# Environment: diamond region (L1 metric), homogeneous Poisson arrivals, Poisson package field\n",
    "# Author: ChatGPT (PyTorch PPO)\n",
    "\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b4e5151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x104e875b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# =========================\n",
    "# Global knobs (easy tuning)\n",
    "# =========================\n",
    "SEED = 42\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# City / motion\n",
    "R = 10.0                 # half \"L1 radius\" of diamond region; feasible points satisfy |x|+|y| <= R\n",
    "V = 1.0                  # speed (distance units per minute)\n",
    "DT = 0.5                 # minutes per step\n",
    "HORIZON_MIN = 480.0      # episode length in minutes (e.g., 4 hours)\n",
    "STEPS_PER_EP = int(HORIZON_MIN / DT)\n",
    "# terminal state: package 全部送完\n",
    "\n",
    "\n",
    "# Demand & revenue\n",
    "LAMBDA = 0.20            # ride arrival rate per minute (Poisson, spatially uniform over region)\n",
    "R_PICK_ALPHA = 0.25      # pickup visibility radius parameter r = alpha * R / sqrt(2) (L1-constraint approx)\n",
    "RT = 8.0                 # per-distance revenue for rides\n",
    "RP = 1.0                 # revenue per delivered package\n",
    "GAMMA_PACK = 0.8         # package spatial intensity (packages per unit area); area = R^2 for diamond in L1 model\n",
    "RIDE_TTL_MINUTES = 3    # ride request time to live in minutes\n",
    "\n",
    "# DRL obs/action shaping\n",
    "MAX_VISIBLE_RIDES = 5    # keep top-K closest pickups\n",
    "K_NEAREST_PACK = 10      # encode nearest K packages\n",
    "DISCOUNT = 0.99\n",
    "PPO_STEPS = 4096\n",
    "PPO_MINI_BATCH = 256\n",
    "PPO_EPOCHS = 4\n",
    "CLIP_EPS = 0.2\n",
    "VF_COEF = 0.5\n",
    "ENT_COEF = 0.01\n",
    "LR = 3e-4\n",
    "UPDATES = 200\n",
    "\n",
    "EVAL_EPISODES = 5        # evaluation batch after each update\n",
    "\n",
    "rng = np.random.default_rng(SEED)\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14955366",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==============\n",
    "# Util functions\n",
    "# ==============\n",
    "def manhattan(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    return float(abs(a[0]-b[0]) + abs(a[1]-b[1]))\n",
    "\n",
    "def l1_inside(x: float, y: float, r: float) -> bool:\n",
    "    return abs(x) + abs(y) <= r\n",
    "\n",
    "def sample_uniform_point_in_diamond(R: float) -> np.ndarray:\n",
    "    # Rejection sampling in square [-R, R]^2 with L1 constraint\n",
    "    while True:\n",
    "        x = rng.uniform(-R, R)\n",
    "        y = rng.uniform(-R, R)\n",
    "        if abs(x) + abs(y) <= R:\n",
    "            return np.array([x, y], dtype=np.float32)\n",
    "\n",
    "# 正常情况来说，不会跑出去菱形范围，此处出于稳健性考虑，如在边界的时候因为数值误差导致跑出去了，则需要拉回来投影到最近的边界\n",
    "def project_to_diamond(p: np.ndarray, R: float) -> np.ndarray:\n",
    "    # If |x|+|y|>R, project to boundary along direction to origin\n",
    "    s = abs(p[0]) + abs(p[1])\n",
    "    if s <= R:\n",
    "        return p\n",
    "    if s == 0:\n",
    "        return np.array([0.0, 0.0], dtype=np.float32)\n",
    "    return p * (R / s)\n",
    "\n",
    "def step_towards(from_pt: np.ndarray, to_pt: np.ndarray, max_dist: float) -> Tuple[np.ndarray, float, bool]:\n",
    "    \"\"\"Move along Manhattan shortest path: first x, then y (or vice versa); return (new_pos, traveled, reached)\"\"\"\n",
    "    x0, y0 = from_pt\n",
    "    x1, y1 = to_pt\n",
    "    dx = abs(x1 - x0)\n",
    "    dy = abs(y1 - y0)\n",
    "    d = dx + dy\n",
    "    if d <= max_dist:\n",
    "        return to_pt.copy(), d, True\n",
    "    # Move along x first\n",
    "    move_x = min(dx, max_dist)\n",
    "    sign_x = np.sign(x1 - x0)\n",
    "    x_new = x0 + sign_x * move_x\n",
    "    remaining = max_dist - move_x\n",
    "    if remaining > 1e-8:\n",
    "        move_y = min(dy, remaining)\n",
    "        sign_y = np.sign(y1 - y0)\n",
    "        y_new = y0 + sign_y * move_y\n",
    "    else:\n",
    "        y_new = y0\n",
    "    p = np.array([x_new, y_new], dtype=np.float32)\n",
    "    return p, max_dist, False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c7d0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ========================\n",
    "# Environment (no zoning)\n",
    "# ========================\n",
    "@dataclass\n",
    "class RideReq:\n",
    "    pickup: np.ndarray\n",
    "    dropoff: np.ndarray\n",
    "    trip_len: float\n",
    "\n",
    "class CoModalEnv:\n",
    "    \"\"\"\n",
    "    - Region: diamond |x|+|y| <= R (L1 metric)\n",
    "    - Packages: spatial Poisson with intensity gamma; N ~ Poisson(gamma * R^2), positions IID uniform in diamond\n",
    "    - Ride arrivals: Poisson(LAMBDA * DT) per step, each with (origin, destination) uniform in diamond\n",
    "    - Vehicle:\n",
    "        * cannot deliver packages while carrying passenger (pickup->dropoff)\n",
    "        * revenue: RP per delivered package; RT * (distance with passenger)\n",
    "    - Action (discrete): 0 = continue delivering package; i=1..MAX_VISIBLE_RIDES = accept ith visible ride (if available)\n",
    "      When busy enroute to pickup/dropoff, action is ignored (auto-continue).\n",
    "    - Observation: vector with normalized features:\n",
    "        * pos (x/R, y/R), time_frac, flags (to_pickup, with_pass)\n",
    "        * K nearest packages: for each, (dx/R, dy/R, l1/R)\n",
    "        * up to K rides visible: for each, (dx_pick/R, dy_pick/R, l1_pick/R, dx_drop/R, dy_drop/R, l1_trip/R)\n",
    "        * counts: remaining_pkg / (1 + E[N]), current_visible / MAX_VISIBLE_RIDES\n",
    "      Invalid slots are zero-padded. We also return an action mask for invalid ride indices.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 R: float = R,\n",
    "                 v: float = V,\n",
    "                 dt: float = DT,\n",
    "                 lam: float = LAMBDA,\n",
    "                 gamma_pack: float = GAMMA_PACK,\n",
    "                 rp: float = RP,\n",
    "                 rt: float = RT,\n",
    "                 r_pick_alpha: float = R_PICK_ALPHA,\n",
    "                 ride_ttl_minutes: float=RIDE_TTL_MINUTES): #ride request time to live in minutes\n",
    "        self.R = R\n",
    "        self.v = v\n",
    "        self.dt = dt\n",
    "        self.lam = lam\n",
    "        self.gamma = gamma_pack\n",
    "        self.rp = rp\n",
    "        self.rt = rt\n",
    "        # max pickup radius (L1) following r = alpha * R / sqrt(2); we keep L1 constraint\n",
    "        self.r_pick = r_pick_alpha * R / math.sqrt(2.0)\n",
    "\n",
    "        self.max_visible = MAX_VISIBLE_RIDES\n",
    "        self.k_pack = K_NEAREST_PACK\n",
    "        self.ride_ttl_steps = max(1, int(round(ride_ttl_minutes / self.dt)))\n",
    "        self._ended_reason: Optional[str] = None\n",
    "        \n",
    "        self.reset()\n",
    "\n",
    "    def reset(self, seed: Optional[int] = None):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        # sample packages\n",
    "        area = self.R ** 2  # for L1 diamond in the paper\n",
    "        n_pkg = rng.poisson(self.gamma * area)\n",
    "        n_pkg = max(1, int(n_pkg))  # ensure non-empty\n",
    "        self.packages = [sample_uniform_point_in_diamond(self.R) for _ in range(n_pkg)]\n",
    "        self.packages = np.array(self.packages, dtype=np.float32)\n",
    "\n",
    "        self.pos = np.array([0.0, 0.0], dtype=np.float32)\n",
    "        self.t = 0.0\n",
    "        self.ride_buffer: List[Tuple[RideReq, int]] = []\n",
    "        self.to_pickup: Optional[np.ndarray] = None\n",
    "        self.with_passenger: bool = False\n",
    "        self.drop_target: Optional[np.ndarray] = None\n",
    "\n",
    "        self.revenue_cum = 0.0\n",
    "        self._steps = 0\n",
    "        obs, mask = self._get_obs()\n",
    "        return obs, mask\n",
    "\n",
    "    def _append_new_ride(self, reqs: List[RideReq]):\n",
    "        \"\"\"Bring the new ride request to this buffer.\"\"\"\n",
    "        if not reqs:\n",
    "            return\n",
    "        ttl0 = self.ride_ttl_steps\n",
    "        self.ride_buffer.extend([(r, ttl0) for r in reqs])\n",
    "\n",
    "    def _sample_rides_this_step(self) -> List[RideReq]:\n",
    "        k = rng.poisson(self.lam * self.dt)\n",
    "        reqs = []\n",
    "        for _ in range(int(k)):\n",
    "            pk = sample_uniform_point_in_diamond(self.R)\n",
    "            dp = sample_uniform_point_in_diamond(self.R)\n",
    "            reqs.append(RideReq(pk, dp, manhattan(pk, dp)))\n",
    "        return reqs\n",
    "\n",
    "    def _visible_rides(self) -> List[RideReq]:\n",
    "        # only show rides with pickup within r_pick (L1) and TTL>0\n",
    "        cand = [r for (r, ttl) in self.ride_buffer\n",
    "                if ttl > 0 and manhattan(self.pos, r.pickup) <= self.r_pick]\n",
    "        cand.sort(key=lambda r: manhattan(self.pos, r.pickup))\n",
    "        return cand[:self.max_visible]\n",
    "\n",
    "    def _nearest_package(self) -> Optional[np.ndarray]:\n",
    "        if len(self.packages) == 0:\n",
    "            return None\n",
    "        dists = np.abs(self.packages - self.pos).sum(axis=1)\n",
    "        idx = int(np.argmin(dists))\n",
    "        return self.packages[idx].copy()\n",
    "\n",
    "\n",
    "    def _deliver_if_arrived(self):\n",
    "        if len(self.packages) == 0:\n",
    "            return 0\n",
    "        # Deliver packages exactly at current location (robust with small threshold)\n",
    "        dists = np.abs(self.packages - self.pos).sum(axis=1)\n",
    "        hit = np.where(dists < 1e-6)[0]\n",
    "        delivered = len(hit)\n",
    "        if delivered > 0:\n",
    "            # deliver all that are exactly here (batch)\n",
    "            self.revenue_cum += self.rp * delivered\n",
    "            self.packages = np.delete(self.packages, hit, axis=0)\n",
    "        return delivered\n",
    "\n",
    "    def _get_obs(self) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        # Visible rides & nearest packages\n",
    "        visible = self._visible_rides()\n",
    "        # Packages features\n",
    "        pack_feats = []\n",
    "        if len(self.packages) > 0:\n",
    "            d = np.abs(self.packages - self.pos).sum(axis=1)\n",
    "            order = np.argsort(d)[:self.k_pack]\n",
    "            for j in order:\n",
    "                rel = (self.packages[j] - self.pos) / self.R\n",
    "                pack_feats.extend([rel[0], rel[1], d[j] / self.R])\n",
    "        # pad\n",
    "        while len(pack_feats) < 3 * self.k_pack:\n",
    "            pack_feats.append(0.0)\n",
    "\n",
    "        ride_feats = []\n",
    "        for r in visible:\n",
    "            relp = (r.pickup - self.pos) / self.R\n",
    "            reld = (r.dropoff - self.pos) / self.R\n",
    "            ride_feats.extend([relp[0], relp[1], manhattan(self.pos, r.pickup) / self.R,\n",
    "                               reld[0], reld[1], r.trip_len / self.R])\n",
    "        while len(ride_feats) < 6 * self.max_visible:\n",
    "            ride_feats.append(0.0)\n",
    "\n",
    "        time_frac = self.t / HORIZON_MIN\n",
    "        flags = [1.0 if self.to_pickup is not None else 0.0,\n",
    "                 1.0 if self.with_passenger else 0.0]\n",
    "\n",
    "        counts = [len(self.packages) / (1.0 + self.gamma * (self.R ** 2)),\n",
    "                  len(visible) / float(self.max_visible)]\n",
    "\n",
    "        core = [self.pos[0] / self.R, self.pos[1] / self.R, time_frac] + flags + counts\n",
    "        obs = np.array(core + pack_feats + ride_feats, dtype=np.float32)\n",
    "\n",
    "        # Action mask: 0 always valid; i>0 valid if i<=len(visible)\n",
    "        mask = np.zeros(1 + self.max_visible, dtype=np.float32)\n",
    "        mask[0] = 1.0\n",
    "        for i in range(len(visible)):\n",
    "            mask[1 + i] = 1.0\n",
    "        return obs, mask\n",
    "\n",
    "    def step(self, action: int):\n",
    "        reward = 0.0\n",
    "        done = False\n",
    "\n",
    "        # add fresh arrivals\n",
    "        self._append_new_ride(self._sample_rides_this_step())\n",
    "\n",
    "        # If busy towards pickup or with passenger, ignore action (auto-continue)\n",
    "        if self.to_pickup is not None:\n",
    "            # move towards pickup\n",
    "            new_pos, d, reached = step_towards(self.pos, self.to_pickup, self.v * self.dt)\n",
    "            self.pos = project_to_diamond(new_pos, self.R)\n",
    "            if reached:\n",
    "                self.to_pickup = None\n",
    "                self.with_passenger = True\n",
    "            # cannot deliver while enroute to pickup\n",
    "        elif self.with_passenger:\n",
    "            # move towards dropoff; accrue ride revenue per distance traveled\n",
    "            new_pos, d, reached = step_towards(self.pos, self.drop_target, self.v * self.dt)\n",
    "            self.pos = project_to_diamond(new_pos, self.R)\n",
    "            reward += self.rt * d    # ride distance revenue in this step\n",
    "            self.revenue_cum += self.rt * d\n",
    "            if reached:\n",
    "                self.with_passenger = False\n",
    "                self.drop_target = None\n",
    "        else:\n",
    "            # free: can choose to deliver or accept a visible ride\n",
    "            visible = self._visible_rides()\n",
    "            if action > 0 and action <= len(visible):\n",
    "                chosen = visible[action - 1]\n",
    "                # remove chosen from buffer\n",
    "                # (remove by identity)\n",
    "                for i, (r, ttl) in enumerate(self.ride_buffer):\n",
    "                    if r is chosen:\n",
    "                        self.ride_buffer.pop(i)\n",
    "                        break\n",
    "                # set pickup/drop targets\n",
    "                self.to_pickup = chosen.pickup.copy()\n",
    "                self.drop_target = chosen.dropoff.copy()\n",
    "                # move towards pickup immediately this step\n",
    "                new_pos, d, reached = step_towards(self.pos, self.to_pickup, self.v * self.dt)\n",
    "                # self.pos = project_to_diamond(new_pos, self.R)\n",
    "                if reached:\n",
    "                    self.to_pickup = None\n",
    "                    self.with_passenger = True\n",
    "            else:\n",
    "                # deliver: go to nearest package (if any)\n",
    "                target = self._nearest_package()\n",
    "                if target is not None:\n",
    "                    new_pos, d, reached = step_towards(self.pos, target, self.v * self.dt)\n",
    "                    self.pos = project_to_diamond(new_pos, self.R)\n",
    "                    if reached:\n",
    "                        delivered = self._deliver_if_arrived()\n",
    "                        reward += self.rp * delivered\n",
    "                # else idle at current location\n",
    "\n",
    "        # small clean-up: remove stale rides outside pickup radius (we keep buffer but they vanish after 1 step) and TTL Decay\n",
    "        # simple model: unaccepted rides expire by end of step\n",
    "        if self.ride_buffer:\n",
    "            new_buf = []\n",
    "            for (r, ttl) in self.ride_buffer:\n",
    "                if ttl > 0:\n",
    "                    new_buf.append((r, ttl))\n",
    "            self.ride_buffer = new_buf\n",
    "\n",
    "        self.t += self.dt\n",
    "        self._steps += 1\n",
    "\n",
    "        # Deliver any packages exactly at position (numerical safety)\n",
    "        if self.to_pickup is None and not self.with_passenger:\n",
    "            delivered = self._deliver_if_arrived()\n",
    "            reward += self.rp * delivered\n",
    "\n",
    "        if self.t >= HORIZON_MIN:\n",
    "            done = True\n",
    "        if (len(self.packages) == 0) and (not self.with_passenger) and (self.to_pickup is None):\n",
    "            # all packages done and not carrying a passenger -> end early\n",
    "            done = True\n",
    "\n",
    "        obs, mask = self._get_obs()\n",
    "        return obs, reward, done, {}, mask\n",
    "\n",
    "    @property\n",
    "    def obs_dim(self):\n",
    "        # core (3 + 2 flags + 2 counts) + 3*K + 6*MAX_VISIBLE\n",
    "        return (3 + 2 + 2) + 3 * self.k_pack + 6 * self.max_visible\n",
    "\n",
    "    @property\n",
    "    def act_dim(self):\n",
    "        return 1 + self.max_visible\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1f5263b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =================\n",
    "# Baseline policies\n",
    "# =================\n",
    "def run_episode(env: CoModalEnv, policy=None, greedy=False) -> float:\n",
    "    obs, mask = env.reset()\n",
    "    total = 0.0\n",
    "    for _ in range(STEPS_PER_EP):\n",
    "        if policy is None:\n",
    "            # default: deliver-only baseline\n",
    "            action = 0\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                o = torch.tensor(obs, dtype=torch.float32, device=DEVICE).unsqueeze(0)\n",
    "                m = torch.tensor(mask, dtype=torch.float32, device=DEVICE).unsqueeze(0)\n",
    "                logits, _ = policy(o)\n",
    "                # mask invalid\n",
    "                invalid = (m < 0.5)\n",
    "                logits = logits.masked_fill(invalid, -1e9)\n",
    "                if greedy:\n",
    "                    action = int(torch.argmax(logits, dim=-1).item())\n",
    "                else:\n",
    "                    probs = torch.softmax(logits, dim=-1)\n",
    "                    action = int(torch.multinomial(probs, 1).item())\n",
    "        obs, r, done, _, mask = env.step(action)\n",
    "        total += r\n",
    "        if done:\n",
    "            break\n",
    "    return total\n",
    "\n",
    "\n",
    "def baseline_nearby_rule(env: CoModalEnv, pickup_alpha=R_PICK_ALPHA, drop_bias=0.5) -> float:\n",
    "    \"\"\"\n",
    "    Heuristic: accept a ride if:\n",
    "      - pickup within r_pick\n",
    "      - and dropoff is closer (L1) than 'drop_bias * nearest remaining package distance'\n",
    "    Else deliver.\n",
    "    \"\"\"\n",
    "    obs, mask = env.reset()\n",
    "    total = 0.0\n",
    "    for _ in range(STEPS_PER_EP):\n",
    "        # decode visible rides\n",
    "        # simple access via env methods\n",
    "        visible = env._visible_rides()\n",
    "        action = 0\n",
    "        if len(visible) > 0:\n",
    "            # nearest package distance\n",
    "            nn = env._nearest_package()\n",
    "            nn_d = manhattan(env.pos, nn) if nn is not None else 0.0\n",
    "            chosen_idx = -1\n",
    "            best_score = -1e9\n",
    "            for i, r in enumerate(visible):\n",
    "                pk_d = manhattan(env.pos, r.pickup)\n",
    "                dp_d = manhattan(env.pos, r.dropoff)\n",
    "                # favor close pickup and dropoff closer than package\n",
    "                score = -pk_d + (nn_d - dp_d) * drop_bias\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    chosen_idx = i\n",
    "            if best_score > 0:\n",
    "                action = 1 + chosen_idx\n",
    "        obs, r, done, _, mask = env.step(action)\n",
    "        total += r\n",
    "        if done:\n",
    "            break\n",
    "    return total\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "239ef0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==============\n",
    "# PPO Components\n",
    "# ==============\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, obs_dim: int, act_dim: int):\n",
    "        super().__init__()\n",
    "        hid = 256\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_dim, hid), nn.ReLU(),\n",
    "            nn.Linear(hid, hid), nn.ReLU(),\n",
    "        )\n",
    "        self.pi = nn.Linear(hid, act_dim)\n",
    "        self.v  = nn.Linear(hid, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        h = self.net(x)\n",
    "        return self.pi(h), self.v(h)\n",
    "\n",
    "\n",
    "def ppo_update(policy: ActorCritic, optimizer, batch, clip_eps=CLIP_EPS):\n",
    "    obs, act, old_logp, ret, adv, mask = batch\n",
    "    logits, v = policy(obs)\n",
    "    # mask invalid actions\n",
    "    invalid = (mask < 0.5)\n",
    "    logits = logits.masked_fill(invalid, -1e9)\n",
    "    dist = torch.distributions.Categorical(logits=logits)\n",
    "    logp = dist.log_prob(act)\n",
    "    ratio = torch.exp(logp - old_logp)\n",
    "\n",
    "    clip_adv = torch.clamp(ratio, 1.0 - clip_eps, 1.0 + clip_eps) * adv\n",
    "    pi_loss = -(torch.min(ratio * adv, clip_adv)).mean()\n",
    "    v_loss = ((ret - v.squeeze(-1)) ** 2).mean()\n",
    "\n",
    "    ent = dist.entropy().mean()\n",
    "    loss = pi_loss + VF_COEF * v_loss - ENT_COEF * ent\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(policy.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "\n",
    "    return pi_loss.item(), v_loss.item(), ent.item(), loss.item()\n",
    "\n",
    "\n",
    "def collect_rollout(env: CoModalEnv, policy: ActorCritic, steps: int):\n",
    "    obs_buf, act_buf, rew_buf, val_buf, logp_buf, mask_buf, done_buf = [], [], [], [], [], [], []\n",
    "    obs, mask = env.reset()\n",
    "    for _ in range(steps):\n",
    "        o = torch.tensor(obs, dtype=torch.float32, device=DEVICE).unsqueeze(0)\n",
    "        m = torch.tensor(mask, dtype=torch.float32, device=DEVICE).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            logits, v = policy(o)\n",
    "            invalid = (m < 0.5)\n",
    "            logits = logits.masked_fill(invalid, -1e9)\n",
    "            dist = torch.distributions.Categorical(logits=logits)\n",
    "            a = dist.sample()\n",
    "            logp = dist.log_prob(a).squeeze(0)\n",
    "            val = v.squeeze(0).squeeze(-1)\n",
    "        obs2, r, done, _, mask2 = env.step(int(a.item()))\n",
    "\n",
    "        obs_buf.append(o.squeeze(0).cpu().numpy())\n",
    "        mask_buf.append(m.squeeze(0).cpu().numpy())\n",
    "        act_buf.append(a.cpu().numpy())\n",
    "        rew_buf.append(r)\n",
    "        val_buf.append(val.cpu().numpy())\n",
    "        logp_buf.append(logp.cpu().numpy())\n",
    "        done_buf.append(done)\n",
    "\n",
    "        obs, mask = obs2, mask2\n",
    "        if done:\n",
    "            obs, mask = env.reset()\n",
    "    # compute GAE-lambda = 1 (pure MC) for simplicity\n",
    "    rets, advs = [], []\n",
    "    ret = 0.0\n",
    "    for r, v, d in zip(reversed(rew_buf), reversed(val_buf), reversed(done_buf)):\n",
    "        if d:\n",
    "            ret = 0.0\n",
    "        ret = r + DISCOUNT * ret\n",
    "        rets.append(ret)\n",
    "        advs.append(ret - v)\n",
    "    rets.reverse(); advs.reverse()\n",
    "    # to tensors\n",
    "    obs_t  = torch.tensor(np.array(obs_buf), dtype=torch.float32, device=DEVICE)\n",
    "    act_t  = torch.tensor(np.array(act_buf).squeeze(-1), dtype=torch.long, device=DEVICE)\n",
    "    ret_t  = torch.tensor(np.array(rets), dtype=torch.float32, device=DEVICE)\n",
    "    adv_t  = torch.tensor(np.array(advs), dtype=torch.float32, device=DEVICE)\n",
    "    logp_t = torch.tensor(np.array(logp_buf), dtype=torch.float32, device=DEVICE)\n",
    "    mask_t = torch.tensor(np.array(mask_buf), dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "    # normalize advantages\n",
    "    adv_t = (adv_t - adv_t.mean()) / (adv_t.std() + 1e-8)\n",
    "    return obs_t, act_t, logp_t, ret_t, adv_t, mask_t\n",
    "\n",
    "\n",
    "def make_minibatches(batch, batch_size):\n",
    "    N = batch[0].shape[0]\n",
    "    idx = np.arange(N)\n",
    "    rng.shuffle(idx)\n",
    "    for i in range(0, N, batch_size):\n",
    "        j = idx[i:i+batch_size]\n",
    "        yield tuple(x[j] for x in batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06a6a16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def evaluate_all(env: CoModalEnv, policy: ActorCritic):\n",
    "    # DRL (greedy)\n",
    "    drl = np.mean([run_episode(env, policy, greedy=True) for _ in range(EVAL_EPISODES)])\n",
    "    # Heuristics\n",
    "    pure = np.mean([run_episode(env, policy=None) for _ in range(EVAL_EPISODES)])\n",
    "    heur = np.mean([baseline_nearby_rule(env) for _ in range(EVAL_EPISODES)])\n",
    "    return drl, heur, pure\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "989356cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    env = CoModalEnv()\n",
    "    policy = ActorCritic(env.obs_dim, env.act_dim).to(DEVICE)\n",
    "    optimizer = optim.Adam(policy.parameters(), lr=LR)\n",
    "\n",
    "    print(f\"Device: {DEVICE}, obs_dim={env.obs_dim}, act_dim={env.act_dim}\")\n",
    "    print(\"Start training PPO...\")\n",
    "    for upd in range(1, UPDATES + 1):\n",
    "        batch = collect_rollout(env, policy, PPO_STEPS)\n",
    "        for _ in range(PPO_EPOCHS):\n",
    "            for mb in make_minibatches(batch, PPO_MINI_BATCH):\n",
    "                pi_l, v_l, ent, tot = ppo_update(policy, optimizer, mb, CLIP_EPS)\n",
    "        if upd % 5 == 0:\n",
    "            drl, heur, pure = evaluate_all(env, policy)\n",
    "            print(f\"[Upd {upd:03d}] DRL={drl:8.2f} | heur={heur:8.2f} | pure={pure:8.2f} \"\n",
    "                  f\"| pi={pi_l:.3f} v={v_l:.3f} ent={ent:.3f}\")\n",
    "\n",
    "    # Final evaluation\n",
    "    drl, heur, pure = evaluate_all(env, policy)\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Final evaluation over {EVAL_EPISODES} eps (4h each):\")\n",
    "    print(f\"  DRL (greedy):        {drl:.2f}\")\n",
    "    print(f\"  Nearby heuristic:    {heur:.2f}\")\n",
    "    print(f\"  Pure delivery only:  {pure:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eccb0f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu, obs_dim=67, act_dim=6\n",
      "Start training PPO...\n",
      "[Upd 005] DRL=   78.20 | heur=  211.53 | pure=   81.60 | pi=0.076 v=7759.324 ent=0.321\n",
      "[Upd 010] DRL=   97.22 | heur=  332.73 | pure=   79.00 | pi=0.037 v=2431.477 ent=0.135\n",
      "[Upd 015] DRL=  170.92 | heur=  154.56 | pure=   83.60 | pi=-0.110 v=1857.303 ent=0.098\n",
      "[Upd 020] DRL=   75.80 | heur=  144.04 | pure=   78.40 | pi=-0.034 v=4255.489 ent=0.250\n",
      "[Upd 025] DRL=   74.40 | heur=  138.26 | pure=   76.40 | pi=-0.032 v=3769.697 ent=0.117\n",
      "[Upd 030] DRL=   85.07 | heur=  160.00 | pure=   78.60 | pi=0.046 v=4597.825 ent=0.176\n",
      "[Upd 035] DRL=   85.58 | heur=  128.93 | pure=   83.00 | pi=0.011 v=3855.145 ent=0.113\n",
      "[Upd 040] DRL=   77.80 | heur=  169.67 | pure=   78.00 | pi=-0.021 v=2650.376 ent=0.092\n",
      "[Upd 045] DRL=   76.40 | heur=  159.16 | pure=   82.20 | pi=-0.007 v=3413.476 ent=0.070\n",
      "[Upd 050] DRL=  100.87 | heur=  247.58 | pure=   86.60 | pi=-0.125 v=4410.538 ent=0.123\n",
      "[Upd 055] DRL=   83.60 | heur=  235.40 | pure=   83.60 | pi=0.061 v=2251.060 ent=0.214\n",
      "[Upd 060] DRL=   86.50 | heur=  202.14 | pure=   88.20 | pi=-0.046 v=3415.890 ent=0.124\n",
      "[Upd 065] DRL=   96.43 | heur=  224.05 | pure=   78.80 | pi=-0.057 v=2644.656 ent=0.110\n",
      "[Upd 070] DRL=   79.00 | heur=  144.27 | pure=   83.40 | pi=-0.048 v=3409.880 ent=0.122\n",
      "[Upd 075] DRL=   79.60 | heur=  109.52 | pure=   85.80 | pi=0.029 v=4042.217 ent=0.199\n",
      "[Upd 080] DRL=   82.00 | heur=   91.44 | pure=   80.60 | pi=-0.032 v=3185.884 ent=0.166\n",
      "[Upd 085] DRL=   78.00 | heur=  135.98 | pure=   80.80 | pi=-0.028 v=4019.224 ent=0.260\n",
      "[Upd 090] DRL=   98.21 | heur=  185.35 | pure=   88.00 | pi=-0.039 v=4087.806 ent=0.307\n",
      "[Upd 095] DRL= 2427.14 | heur=  153.51 | pure=   78.20 | pi=-0.097 v=3710.126 ent=0.332\n",
      "[Upd 100] DRL=   92.57 | heur=  119.77 | pure=   80.20 | pi=-0.009 v=3547.403 ent=0.277\n",
      "[Upd 105] DRL=   76.80 | heur=  252.76 | pure=   85.20 | pi=0.105 v=4701.051 ent=0.307\n",
      "[Upd 110] DRL=   83.60 | heur=  221.74 | pure=   81.00 | pi=-0.043 v=3579.909 ent=0.339\n",
      "[Upd 115] DRL=   92.33 | heur=  143.11 | pure=   80.20 | pi=0.002 v=3067.117 ent=0.313\n",
      "[Upd 120] DRL=   85.00 | heur=  198.08 | pure=   80.40 | pi=0.028 v=4055.824 ent=0.290\n",
      "[Upd 125] DRL=   74.40 | heur=  204.28 | pure=   76.60 | pi=0.014 v=2847.840 ent=0.292\n",
      "[Upd 130] DRL=  100.27 | heur=  153.40 | pure=   76.00 | pi=0.025 v=2406.507 ent=0.271\n",
      "[Upd 135] DRL=  305.72 | heur=  222.70 | pure=   77.40 | pi=0.046 v=2670.632 ent=0.312\n",
      "[Upd 140] DRL=  613.16 | heur=  230.06 | pure=   82.40 | pi=0.066 v=1569.781 ent=0.297\n",
      "[Upd 145] DRL=  128.83 | heur=  144.34 | pure=   78.80 | pi=0.040 v=3562.395 ent=0.327\n",
      "[Upd 150] DRL=  479.01 | heur=  162.82 | pure=   82.60 | pi=0.063 v=2161.871 ent=0.316\n",
      "[Upd 155] DRL= 1155.81 | heur=  187.47 | pure=   84.80 | pi=-0.086 v=1958.522 ent=0.315\n",
      "[Upd 160] DRL= 1148.03 | heur=  105.85 | pure=   71.60 | pi=0.056 v=2199.529 ent=0.315\n",
      "[Upd 165] DRL=  835.52 | heur=  136.12 | pure=   83.40 | pi=-0.075 v=1720.937 ent=0.347\n",
      "[Upd 170] DRL=  208.66 | heur=  216.42 | pure=   85.40 | pi=-0.005 v=1651.404 ent=0.378\n",
      "[Upd 175] DRL= 2510.85 | heur=  142.34 | pure=   76.80 | pi=0.035 v=1694.683 ent=0.401\n",
      "[Upd 180] DRL= 2574.95 | heur=  145.36 | pure=   79.60 | pi=0.077 v=1249.814 ent=0.274\n",
      "[Upd 185] DRL= 2685.72 | heur=   74.20 | pure=   78.40 | pi=0.088 v=1007.406 ent=0.412\n",
      "[Upd 190] DRL= 2615.72 | heur=  123.44 | pure=   72.60 | pi=0.013 v=1372.386 ent=0.228\n",
      "[Upd 195] DRL= 2537.95 | heur=  158.71 | pure=   75.40 | pi=-0.025 v=1074.344 ent=0.260\n",
      "[Upd 200] DRL= 2558.47 | heur=  175.09 | pure=   80.00 | pi=0.072 v=1787.966 ent=0.215\n",
      "================================================================================\n",
      "Final evaluation over 5 eps (4h each):\n",
      "  DRL (greedy):        2507.45\n",
      "  Nearby heuristic:    164.14\n",
      "  Pure delivery only:  80.00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
